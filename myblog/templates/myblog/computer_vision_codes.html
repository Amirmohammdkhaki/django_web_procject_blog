{% extends '_base.html' %}
{% load static %}

{% block head_title %}Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯Ù‡Ø§ÛŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± - ÙˆØ¨Ù„Ø§Ú¯ Ø§Ù…ÛŒØ±Ù…Ø­Ù…Ø¯{% endblock %}

{% block breadcrumb_items %}
<li class="breadcrumb-item"><a href="{% url 'projects' %}">Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§</a></li>
<li class="breadcrumb-item active" aria-current="page">Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯Ù‡Ø§ÛŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±</li>
{% endblock %}

{% block content %}
<div class="container my-5">
    <div class="row">
        <div class="col-12 text-center mb-5">
            <h1 class="display-4 fw-bold text-primary mb-3">ğŸ¯ Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯Ù‡Ø§ÛŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±</h1>
            <p class="lead">ğŸ“š Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¬Ø§Ù…Ø¹ Ú©Ø¯Ù‡Ø§ÛŒ OpenCVØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± Ùˆ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø§ Python</p>
        </div>
    </div>

    <!-- Ø¨Ø®Ø´ Ù…Ø¹Ø±ÙÛŒ -->
    <div class="row mb-5">
        <div class="col-12">
            <div class="card bg-gradient-primary text-white shadow-lg">
                <div class="card-body text-center py-4">
                    <h3 class="fw-bold mb-3">ğŸš€ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±</h3>
                    <p class="mb-0">Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø² Ú©Ø¯Ù‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ùˆ ØªÙˆØ³Ø¹Ù‡ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±</p>
                </div>
            </div>
        </div>
    </div>

    <div class="row">
        <div class="col-lg-10 mx-auto">
            <!-- Ø¨Ø®Ø´ 1: ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ -->
            <div class="card shadow-lg mb-5">
                <div class="card-header bg-dark text-white">
                    <h4 class="mb-0"><i class="bi bi-person-check me-2"></i>ğŸ‘¤ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ OpenCV</h4>
                </div>
                <div class="card-body">
                    <p class="text-muted mb-4">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù</p>
                    
                    <pre class="bg-light p-4 rounded" style="direction: ltr; text-align: left; max-height: 400px; overflow-y: auto;"><code class="language-python">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced Face Detection System
Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¨Ø§ OpenCV
"""

import cv2
import numpy as np
import os
import time
from datetime import datetime
import json

class AdvancedFaceDetector:
    """Ú©Ù„Ø§Ø³ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ùˆ Ø¢Ù†Ø§Ù„ÛŒØ² Ú†Ù‡Ø±Ù‡"""
    
    def __init__(self):
        # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        self.profile_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_profileface.xml'
        )
        self.eye_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_eye.xml'
        )
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÛŒØ³ØªÙ…
        self.detection_history = []
        self.face_count = 0
        self.start_time = time.time()
        
    def detect_faces_advanced(self, frame):
        """ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø®Ø§Ú©Ø³ØªØ±ÛŒ
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ù†ØªØ±Ø§Ø³Øª ØªØµÙˆÛŒØ±
        gray = cv2.equalizeHist(gray)
        
        # ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø§Ø² Ù†Ù…Ø§ÛŒ frontal
        faces_frontal = self.face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(30, 30),
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        
        # ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø§Ø² Ù†Ù…Ø§ÛŒ profile
        faces_profile = self.profile_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(30, 30)
        )
        
        # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬
        all_faces = list(faces_frontal) + list(faces_profile)
        
        return all_faces, gray
    
    def draw_detection_info(self, frame, faces, gray):
        """Ø±Ø³Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ´Ø®ÛŒØµ Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±"""
        for i, (x, y, w, h) in enumerate(faces):
            # Ø±Ø³Ù… Ù…Ø³ØªØ·ÛŒÙ„ Ø¯ÙˆØ± Ú†Ù‡Ø±Ù‡
            color = (0, 255, 0)  # Ø³Ø¨Ø²
            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø´Ù†Ø§Ø³Ù‡
            face_id = f"Face {i+1}"
            cv2.putText(frame, face_id, (x, y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            # ØªØ´Ø®ÛŒØµ Ú†Ø´Ù…â€ŒÙ‡Ø§ Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ú†Ù‡Ø±Ù‡
            roi_gray = gray[y:y+h, x:x+w]
            eyes = self.eye_cascade.detectMultiScale(roi_gray)
            
            for (ex, ey, ew, eh) in eyes:
                cv2.rectangle(frame, (x+ex, y+ey), (x+ex+ew, y+ey+eh), (255, 0, 0), 1)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø§Ø¨Ø¹Ø§Ø¯ Ú†Ù‡Ø±Ù‡
            face_size = f"Size: {w}x{h}"
            cv2.putText(frame, face_size, (x, y+h+20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ´Ø®ÛŒØµ
            self._save_detection_data(x, y, w, h)
    
    def _save_detection_data(self, x, y, w, h):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ"""
        detection_data = {
            'timestamp': datetime.now().isoformat(),
            'position': {'x': x, 'y': y, 'width': w, 'height': h},
            'face_id': self.face_count + 1
        }
        self.detection_history.append(detection_data)
        self.face_count += 1
    
    def get_statistics(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø³ÛŒØ³ØªÙ…"""
        current_time = time.time()
        runtime = current_time - self.start_time
        
        stats = {
            'total_faces_detected': self.face_count,
            'runtime_seconds': round(runtime, 2),
            'detections_per_minute': round(self.face_count / (runtime / 60), 2),
            'last_detection': self.detection_history[-1] if self.detection_history else None
        }
        
        return stats
    
    def save_detection_report(self, filename='face_detection_report.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ ØªØ´Ø®ÛŒØµ"""
        report = {
            'system_info': {
                'version': '1.0',
                'created_at': datetime.now().isoformat(),
                'total_detections': self.face_count
            },
            'detection_history': self.detection_history,
            'statistics': self.get_statistics()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡"""
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡...")
    print("=" * 50)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø´ÛŒØ¡ ØªØ´Ø®ÛŒØµâ€ŒØ¯Ù‡Ù†Ø¯Ù‡
    detector = AdvancedFaceDetector()
    
    # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÙˆØ±Ø¨ÛŒÙ†
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        print("âŒ Ø®Ø·Ø§: Ø§Ù…Ú©Ø§Ù† Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯!")
        return
    
    print("âœ… Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    print("ğŸ“ Ø±Ø§Ù‡Ù†Ù…Ø§: Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ø§Ø² Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú©Ù„ÛŒØ¯ 'q' Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯")
    print("-" * 50)
    
    try:
        while True:
            # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ±ÛŒÙ… Ø§Ø² Ø¯ÙˆØ±Ø¨ÛŒÙ†
            ret, frame = cap.read()
            
            if not ret:
                print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ±ÛŒÙ… Ø§Ø² Ø¯ÙˆØ±Ø¨ÛŒÙ†")
                break
            
            # ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§
            faces, gray = detector.detect_faces_advanced(frame)
            
            # Ø±Ø³Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±
            detector.draw_detection_info(frame, faces, gray)
            
            # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ
            stats = detector.get_statistics()
            stats_text = [
                f"Faces Detected: {len(faces)}",
                f"Total Faces: {stats['total_faces_detected']}",
                f"Runtime: {stats['runtime_seconds']}s"
            ]
            
            for i, text in enumerate(stats_text):
                y_position = 30 + (i * 25)
                cv2.putText(frame, text, (10, y_position),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            
            # Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ±
            cv2.imshow('Advanced Face Detection System', frame)
            
            # Ø®Ø±ÙˆØ¬ Ø¨Ø§ ÙØ´Ø§Ø± Ø¯Ø§Ø¯Ù† Ú©Ù„ÛŒØ¯ 'q'
            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("\nğŸ›‘ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ… ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø±...")
                break
                
    except KeyboardInterrupt:
        print("\nğŸ›‘ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ…...")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…: {e}")
    finally:
        # Ø¢Ø²Ø§Ø¯ Ú©Ø±Ø¯Ù† Ù…Ù†Ø§Ø¨Ø¹
        cap.release()
        cv2.destroyAllWindows()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
        detector.save_detection_report()
        print("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ ØªØ´Ø®ÛŒØµ Ø¯Ø± ÙØ§ÛŒÙ„ 'face_detection_report.json' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
        final_stats = detector.get_statistics()
        print("\nğŸ“ˆ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ…:")
        print(f"   ğŸ‘¥ Ú©Ù„ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: {final_stats['total_faces_detected']}")
        print(f"   â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {final_stats['runtime_seconds']} Ø«Ø§Ù†ÛŒÙ‡")
        print(f"   ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ´Ø®ÛŒØµ Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡: {final_stats['detections_per_minute']}")
        print("=" * 50)
        print("âœ… Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®Ø§ØªÙ…Ù‡ ÛŒØ§ÙØª")

if __name__ == "__main__":
    main()
                    </code></pre>
                </div>
            </div>

            <!-- Ø¨Ø®Ø´ 2: ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ -->
            <div class="card shadow-lg mb-5">
                <div class="card-header bg-info text-white">
                    <h4 class="mb-0"><i class="bi bi-filter-circle me-2"></i>ğŸ¨ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡</h4>
                </div>
                <div class="card-body">
                    <p class="text-muted mb-4">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†ÙˆØ§Ø¹ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ùˆ Ø§ÙÚ©Øªâ€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ</p>
                    
                    <pre class="bg-light p-4 rounded" style="direction: ltr; text-align: left; max-height: 400px; overflow-y: auto;"><code class="language-python">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced Image Filters and Effects
ÙÛŒÙ„ØªØ±Ù‡Ø§ Ùˆ Ø§ÙÚ©Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªØµÙˆÛŒØ±ÛŒ
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from enum import Enum

class FilterType(Enum):
    """Ø§Ù†ÙˆØ§Ø¹ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
    GAUSSIAN_BLUR = "gaussian_blur"
    MEDIAN_BLUR = "median_blur"
    BILATERAL_FILTER = "bilateral"
    SHARPEN = "sharpen"
    EDGE_DETECTION = "edge_detection"
    EMBOSS = "emboss"
    SEPIA = "sepia"
    CARTOON = "cartoon"
    SKETCH = "sketch"
    OIL_PAINTING = "oil_painting"

class AdvancedImageProcessor:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªØµÙˆÛŒØ±"""
    
    def __init__(self):
        self.filter_history = []
        self.performance_stats = {}
    
    def apply_gaussian_blur(self, image, kernel_size=15, sigma=0):
        """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ú¯ÙˆØ³ÛŒ"""
        if kernel_size % 2 == 0:
            kernel_size += 1
            
        blurred = cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)
        return blurred
    
    def apply_median_blur(self, image, kernel_size=15):
        """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ù…ÛŒØ§Ù†Ù‡"""
        if kernel_size % 2 == 0:
            kernel_size += 1
            
        blurred = cv2.medianBlur(image, kernel_size)
        return blurred
    
    def apply_bilateral_filter(self, image, d=15, sigma_color=75, sigma_space=75):
        """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ø¯ÙˆØ·Ø±ÙÙ‡ Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ù„Ø¨Ù‡â€ŒÙ‡Ø§"""
        filtered = cv2.bilateralFilter(image, d, sigma_color, sigma_space)
        return filtered
    
    def apply_sharpen_filter(self, image, strength=1.0):
        """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ø´Ø§Ø±Ù¾Ù†"""
        kernel = np.array([[-1, -1, -1],
                          [-1,  9, -1],
                          [-1, -1, -1]]) * strength
        sharpened = cv2.filter2D(image, -1, kernel)
        return sharpened
    
    def apply_edge_detection(self, image, low_threshold=50, high_threshold=150):
        """ØªØ´Ø®ÛŒØµ Ù„Ø¨Ù‡ Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Canny"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, low_threshold, high_threshold)
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØµÙˆÛŒØ± Ø±Ù†Ú¯ÛŒ
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        return edges_colored
    
    def apply_emboss_filter(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ø§Ù…Ø¨Ø§Ø³ (Ø¨Ø±Ø¬Ø³ØªÙ‡)"""
        kernel = np.array([[-2, -1, 0],
                          [-1,  1, 1],
                          [ 0,  1, 2]])
        embossed = cv2.filter2D(image, -1, kernel)
        return embossed
    
    def apply_sepia_filter(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ø³Ù¾ÛŒØ§ (Ø¹Ú©Ø³ Ù‚Ø¯ÛŒÙ…ÛŒ)"""
        sepia_filter = np.array([[0.272, 0.534, 0.131],
                                [0.349, 0.686, 0.168],
                                [0.393, 0.769, 0.189]])
        sepia = cv2.transform(image, sepia_filter)
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ 0-255
        sepia = np.clip(sepia, 0, 255)
        return sepia.astype(np.uint8)
    
    def apply_cartoon_effect(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ú©Ø§Ø±ØªÙˆÙ†ÛŒ"""
        # Ú©Ø§Ù‡Ø´ Ù†ÙˆÛŒØ²
        color = cv2.bilateralFilter(image, 9, 250, 250)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø®Ø§Ú©Ø³ØªØ±ÛŒ
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ù…ÛŒØ§Ù†Ù‡
        gray = cv2.medianBlur(gray, 7)
        
        # ØªØ´Ø®ÛŒØµ Ù„Ø¨Ù‡
        edges = cv2.adaptiveThreshold(gray, 255, 
                                    cv2.ADAPTIVE_THRESH_MEAN_C, 
                                    cv2.THRESH_BINARY, 9, 2)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ù„Ø¨Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØªØµÙˆÛŒØ± Ø±Ù†Ú¯ÛŒ
        edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        
        # ØªØ±Ú©ÛŒØ¨ Ø¨Ø§ ØªØµÙˆÛŒØ± Ø±Ù†Ú¯ÛŒ
        cartoon = cv2.bitwise_and(color, edges)
        return cartoon
    
    def apply_sketch_effect(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ø·Ø±Ø§Ø­ÛŒ Ù…Ø¯Ø§Ø¯"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Ù…Ø¹Ú©ÙˆØ³ Ú©Ø±Ø¯Ù† ØªØµÙˆÛŒØ±
        inverted = 255 - gray
        
        # Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ú¯ÙˆØ³ÛŒ
        blurred = cv2.GaussianBlur(inverted, (21, 21), 0)
        
        # ØªØ±Ú©ÛŒØ¨ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø§ÙÚ©Øª Ø·Ø±Ø§Ø­ÛŒ
        sketch = cv2.divide(gray, 255 - blurred, scale=256)
        sketch_colored = cv2.cvtColor(sketch, cv2.COLOR_GRAY2BGR)
        
        return sketch_colored
    
    def apply_oil_painting_effect(self, image, radius=5, levels=20):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ù†Ù‚Ø§Ø´ÛŒ Ø±ÙˆØºÙ†ÛŒ"""
        h, w = image.shape[:2]
        oil_painting = np.zeros_like(image)
        
        for i in range(radius, h - radius):
            for j in range(radius, w - radius):
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ø­ÛŒÙ‡
                region = image[i-radius:i+radius+1, j-radius:j+radius+1]
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø§Ù†Ø§Ù„
                intensity_counts = np.zeros(levels)
                avg_color = np.zeros(3)
                
                for x in range(region.shape[0]):
                    for y in range(region.shape[1]):
                        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Øª
                        intensity = int(np.mean(region[x, y]) * (levels - 1) / 255)
                        intensity_counts[intensity] += 1
                        avg_color += region[x, y]
                
                # ÛŒØ§ÙØªÙ† Ø´Ø¯Øª ØºØ§Ù„Ø¨
                dominant_intensity = np.argmax(intensity_counts)
                
                # ØªÙ†Ø¸ÛŒÙ… Ø±Ù†Ú¯ Ù¾ÛŒÚ©Ø³Ù„
                oil_painting[i, j] = avg_color / np.sum(intensity_counts)
        
        return oil_painting.astype(np.uint8)
    
    def create_filter_comparison(self, image, filter_types):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨ÛŒÙ† ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        fig, axes = plt.subplots(2, 5, figsize=(20, 8))
        axes = axes.ravel()
        
        # ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ
        axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        axes[0].set_title('Original Image')
        axes[0].axis('off')
        
        # Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ±Ù‡Ø§
        for i, filter_type in enumerate(filter_types, 1):
            filtered_image = self.apply_filter(image, filter_type)
            axes[i].imshow(cv2.cvtColor(filtered_image, cv2.COLOR_BGR2RGB))
            axes[i].set_title(filter_type.value.replace('_', ' ').title())
            axes[i].axis('off')
        
        plt.tight_layout()
        return fig
    
    def apply_filter(self, image, filter_type):
        """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹"""
        filter_methods = {
            FilterType.GAUSSIAN_BLUR: lambda img: self.apply_gaussian_blur(img),
            FilterType.MEDIAN_BLUR: lambda img: self.apply_median_blur(img),
            FilterType.BILATERAL_FILTER: lambda img: self.apply_bilateral_filter(img),
            FilterType.SHARPEN: lambda img: self.apply_sharpen_filter(img),
            FilterType.EDGE_DETECTION: lambda img: self.apply_edge_detection(img),
            FilterType.EMBOSS: lambda img: self.apply_emboss_filter(img),
            FilterType.SEPIA: lambda img: self.apply_sepia_filter(img),
            FilterType.CARTOON: lambda img: self.apply_cartoon_effect(img),
            FilterType.SKETCH: lambda img: self.apply_sketch_effect(img),
            FilterType.OIL_PAINTING: lambda img: self.apply_oil_painting_effect(img)
        }
        
        return filter_methods[filter_type](image)

def demonstrate_filters():
    """Ù†Ù…Ø§ÛŒØ´ Ø¹Ù…Ù„Ú©Ø±Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
    print("ğŸ¨ Ø´Ø±ÙˆØ¹ Ù†Ù…Ø§ÛŒØ´ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ...")
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±
    processor = AdvancedImageProcessor()
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡
    image = cv2.imread('sample_image.jpg')
    
    if image is None:
        # Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
        print("ğŸ“ Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡...")
        image = np.random.randint(0, 255, (400, 600, 3), dtype=np.uint8)
        cv2.imwrite('sample_image.jpg', image)
    
    # Ù„ÛŒØ³Øª ÙÛŒÙ„ØªØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´
    filters_to_show = [
        FilterType.GAUSSIAN_BLUR,
        FilterType.MEDIAN_BLUR,
        FilterType.BILATERAL_FILTER,
        FilterType.SHARPEN,
        FilterType.EDGE_DETECTION,
        FilterType.EMBOSS,
        FilterType.SEPIA,
        FilterType.CARTOON,
        FilterType.SKETCH
    ]
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ù‚Ø§ÛŒØ³Ù‡
    print("ğŸ–¼ï¸ Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ù‚Ø§ÛŒØ³Ù‡ ÙÛŒÙ„ØªØ±Ù‡Ø§...")
    fig = processor.create_filter_comparison(image, filters_to_show)
    plt.savefig('filter_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… Ù…Ù‚Ø§ÛŒØ³Ù‡ ÙÛŒÙ„ØªØ±Ù‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ 'filter_comparison.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø§ÙÚ©Øª Ú©Ø§Ø±ØªÙˆÙ†ÛŒ
    cartoon = processor.apply_cartoon_effect(image)
    cv2.imwrite('cartoon_effect.jpg', cartoon)
    print("ğŸ­ Ø§ÙÚ©Øª Ú©Ø§Ø±ØªÙˆÙ†ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ 'cartoon_effect.jpg' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø§ÙÚ©Øª Ø·Ø±Ø§Ø­ÛŒ
    sketch = processor.apply_sketch_effect(image)
    cv2.imwrite('sketch_effect.jpg', sketch)
    print("âœï¸ Ø§ÙÚ©Øª Ø·Ø±Ø§Ø­ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ 'sketch_effect.jpg' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

if __name__ == "__main__":
    demonstrate_filters()
                    </code></pre>
                </div>
            </div>

            <!-- Ø¨Ø®Ø´ 3: Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª -->
            <div class="card shadow-lg mb-5">
                <div class="card-header bg-warning text-dark">
                    <h4 class="mb-0"><i class="bi bi-activity me-2"></i>ğŸƒâ€â™‚ï¸ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯</h4>
                </div>
                <div class="card-body">
                    <p class="text-muted mb-4">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù†Ø¸Ø§Ø±Øª ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª</p>
                    
                    <pre class="bg-light p-4 rounded" style="direction: ltr; text-align: left; max-height: 400px; overflow-y: auto;"><code class="language-python">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Intelligent Motion Detection System
Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª
"""

import cv2
import numpy as np
import time
import json
from datetime import datetime, timedelta
from collections import deque
import threading

class MotionAnalyzer:
    """Ø¢Ù†Ø§Ù„Ø§ÛŒØ²Ø± Ø­Ø±Ú©Øª Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø­Ø±Ú©Øª Ø¯Ø± ÙˆÛŒØ¯ÛŒÙˆ"""
    
    def __init__(self, min_area=500, threshold=25, history_length=100):
        self.min_area = min_area
        self.threshold = threshold
        self.history_length = history_length
        
        # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø­Ø±Ú©Øª
        self.motion_history = deque(maxlen=history_length)
        self.motion_events = []
        
        # background subtractor
        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(
            history=500, 
            varThreshold=16, 
            detectShadows=True
        )
        
        # Ø¢Ù…Ø§Ø± Ø³ÛŒØ³ØªÙ…
        self.motion_count = 0
        self.start_time = time.time()
        self.last_motion_time = None
        
    def detect_motion(self, frame):
        """ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ø¯Ø± ÙØ±ÛŒÙ… ÙØ¹Ù„ÛŒ"""
        # Ø§Ø¹Ù…Ø§Ù„ background subtraction
        fg_mask = self.bg_subtractor.apply(frame)
        
        # Ø­Ø°Ù Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§
        _, fg_mask = cv2.threshold(fg_mask, 244, 255, cv2.THRESH_BINARY)
        
        # Ø­Ø°Ù Ù†ÙˆÛŒØ²
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)
        
        return fg_mask
    
    def analyze_motion_regions(self, frame, motion_mask):
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ù†ÙˆØ§Ø­ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ø­Ø±Ú©Øª"""
        # ÛŒØ§ÙØªÙ† Ú©Ø§Ù†ØªÙˆØ±Ù‡Ø§
        contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        motion_regions = []
        motion_detected = False
        
        for contour in contours:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø³Ø§Ø­Øª Ú©Ø§Ù†ØªÙˆØ±
            area = cv2.contourArea(contour)
            
            if area > self.min_area:
                motion_detected = True
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø³ØªØ·ÛŒÙ„ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†Ù†Ø¯Ù‡
                x, y, w, h = cv2.boundingRect(contour)
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù†Ø·Ù‚Ù‡
                region_info = {
                    'bbox': (x, y, w, h),
                    'area': area,
                    'center': (x + w//2, y + h//2)
                }
                motion_regions.append(region_info)
                
                # Ø±Ø³Ù… Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±
                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.putText(frame, f"Motion", (x, y-10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # Ø«Ø¨Øª Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø­Ø±Ú©Øª
        if motion_detected:
            self._record_motion_event(motion_regions)
        
        return motion_regions, motion_detected
    
    def _record_motion_event(self, regions):
        """Ø«Ø¨Øª Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø­Ø±Ú©Øª"""
        event = {
            'timestamp': datetime.now().isoformat(),
            'regions': regions,
            'region_count': len(regions),
            'total_area': sum(region['area'] for region in regions)
        }
        
        self.motion_events.append(event)
        self.motion_count += 1
        self.last_motion_time = time.time()
        
        # Ø§Ø¶Ø§ÙÙ‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self.motion_history.append({
            'time': time.time(),
            'motion_detected': True,
            'region_count': len(regions)
        })
    
    def get_motion_statistics(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø­Ø±Ú©Øª"""
        current_time = time.time()
        runtime = current_time - self.start_time
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ¹Ø§Ù„ÛŒØª Ø§Ø®ÛŒØ± (5 Ø¯Ù‚ÛŒÙ‚Ù‡ Ú¯Ø°Ø´ØªÙ‡)
        recent_threshold = current_time - 300  # 5 Ø¯Ù‚ÛŒÙ‚Ù‡
        recent_events = [e for e in self.motion_history 
                        if e['time'] > recent_threshold]
        
        stats = {
            'total_motion_events': self.motion_count,
            'runtime_minutes': round(runtime / 60, 2),
            'events_per_minute': round(self.motion_count / (runtime / 60), 2),
            'recent_activity': len(recent_events),
            'last_motion_time': self.last_motion_time,
            'current_status': 'ACTIVE' if self.last_motion_time and 
                                (current_time - self.last_motion_time) < 60 else 'IDLE'
        }
        
        return stats
    
    def draw_analytics_overlay(self, frame, stats):
        """Ø±Ø³Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù†Ø§Ù„ÛŒØªÛŒÚ©Ø³ Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±"""
        # Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ù†ÛŒÙ…Ù‡ Ø´ÙØ§Ù Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (300, 150), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±
        texts = [
            f"Motion Events: {stats['total_motion_events']}",
            f"Runtime: {stats['runtime_minutes']} min",
            f"Events/Min: {stats['events_per_minute']}",
            f"Recent Activity: {stats['recent_activity']}",
            f"Status: {stats['current_status']}"
        ]
        
        for i, text in enumerate(texts):
            y_pos = 40 + (i * 25)
            cv2.putText(frame, text, (20, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Ù†Ù…Ø§ÛŒØ´ timeline ÙØ¹Ø§Ù„ÛŒØª
        self._draw_activity_timeline(frame, stats)
    
    def _draw_activity_timeline(self, frame, stats):
        """Ø±Ø³Ù… timeline ÙØ¹Ø§Ù„ÛŒØª"""
        timeline_height = 50
        timeline_width = 200
        timeline_x = frame.shape[1] - timeline_width - 20
        timeline_y = 20
        
        # Ø±Ø³Ù… Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ timeline
        cv2.rectangle(frame, 
                     (timeline_x, timeline_y),
                     (timeline_x + timeline_width, timeline_y + timeline_height),
                     (50, 50, 50), -1)
        
        # Ø±Ø³Ù… ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±
        if self.motion_history:
            max_time = max(event['time'] for event in self.motion_history)
            min_time = min(event['time'] for event in self.motion_history)
            time_range = max_time - min_time if max_time != min_time else 1
            
            for event in self.motion_history:
                if event['motion_detected']:
                    x_pos = timeline_x + int(
                        (event['time'] - min_time) / time_range * timeline_width
                    )
                    intensity = min(event['region_count'] * 2, 10)
                    cv2.line(frame,
                            (x_pos, timeline_y),
                            (x_pos, timeline_y + timeline_height),
                            (0, 255, 0), intensity)
        
        cv2.putText(frame, "Activity Timeline", 
                   (timeline_x, timeline_y - 5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)

class IntelligentMotionDetectionSystem:
    """Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯"""
    
    def __init__(self):
        self.motion_analyzer = MotionAnalyzer()
        self.is_running = False
        self.alert_callback = None
        
    def start_monitoring(self, video_source=0, alert_callback=None):
        """Ø´Ø±ÙˆØ¹ Ù†Ø¸Ø§Ø±Øª"""
        self.alert_callback = alert_callback
        self.is_running = True
        
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯...")
        print("ğŸ“Š Ø³ÛŒØ³ØªÙ… Ø¯Ø± Ø­Ø§Ù„ Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ø­Ø±Ú©Øª...")
        
        cap = cv2.VideoCapture(video_source)
        
        if not cap.isOpened():
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù…Ù†Ø¨Ø¹ ÙˆÛŒØ¯ÛŒÙˆ")
            return
        
        try:
            while self.is_running:
                ret, frame = cap.read()
                
                if not ret:
                    print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ±ÛŒÙ…")
                    break
                
                # ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª
                motion_mask = self.motion_analyzer.detect_motion(frame)
                motion_regions, motion_detected = self.motion_analyzer.analyze_motion_regions(
                    frame, motion_mask
                )
                
                # Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø±
                stats = self.motion_analyzer.get_motion_statistics()
                
                # Ø±Ø³Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª
                self.motion_analyzer.draw_analytics_overlay(frame, stats)
                
                # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
                cv2.imshow('Intelligent Motion Detection', frame)
                cv2.imshow('Motion Mask', motion_mask)
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø´Ø¯Ø§Ø±
                if motion_detected and self.alert_callback:
                    self.alert_callback(motion_regions)
                
                # Ø®Ø±ÙˆØ¬ Ø¨Ø§ Ú©Ù„ÛŒØ¯ 'q'
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print("\nğŸ›‘ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ… ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø±...")
                    break
                    
        except KeyboardInterrupt:
            print("\nğŸ›‘ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ…...")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…: {e}")
        finally:
            cap.release()
            cv2.destroyAllWindows()
            self.is_running = False
            
            # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
            final_stats = self.motion_analyzer.get_motion_statistics()
            self._print_final_report(final_stats)
    
    def _print_final_report(self, stats):
        """Ú†Ø§Ù¾ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ"""
        print("\n" + "="*50)
        print("ğŸ“ˆ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª")
        print("="*50)
        print(f"   ğŸ¯ Ú©Ù„ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ø­Ø±Ú©Øª: {stats['total_motion_events']}")
        print(f"   â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {stats['runtime_minutes']} Ø¯Ù‚ÛŒÙ‚Ù‡")
        print(f"   ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡: {stats['events_per_minute']}")
        print(f"   ğŸ”¥ ÙØ¹Ø§Ù„ÛŒØª Ø§Ø®ÛŒØ±: {stats['recent_activity']} Ø±ÙˆÛŒØ¯Ø§Ø¯")
        print(f"   ğŸš¦ ÙˆØ¶Ø¹ÛŒØª Ù†Ù‡Ø§ÛŒÛŒ: {stats['current_status']}")
        print("="*50)
    
    def stop_monitoring(self):
        """ØªÙˆÙ‚Ù Ù†Ø¸Ø§Ø±Øª"""
        self.is_running = False

def motion_alert_handler(motion_regions):
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø­Ø±Ú©Øª"""
    print(f"ğŸš¨ Ù‡Ø´Ø¯Ø§Ø±! Ø­Ø±Ú©Øª ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ - {len(motion_regions)} Ù…Ù†Ø·Ù‚Ù‡")
    
    for i, region in enumerate(motion_regions):
        print(f"   ğŸ“ Ù…Ù†Ø·Ù‚Ù‡ {i+1}: Ù…ÙˆÙ‚Ø¹ÛŒØª {region['center']}, Ù…Ø³Ø§Ø­Øª {region['area']}")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
    print("ğŸƒâ€â™‚ï¸ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯")
    print("ğŸ“ Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ… Ø­Ø±Ú©Øª Ø±Ø§ Ø¯Ø± ÙˆÛŒØ¯ÛŒÙˆ ØªØ´Ø®ÛŒØµ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯")
    
    system = IntelligentMotionDetectionSystem()
    
    # Ø´Ø±ÙˆØ¹ Ù†Ø¸Ø§Ø±Øª Ø¨Ø§ callback Ù‡Ø´Ø¯Ø§Ø±
    system.start_monitoring(
        video_source=0,  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        alert_callback=motion_alert_handler
    )

if __name__ == "__main__":
    main()
                    </code></pre>
                </div>
            </div>

            <!-- Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø¯Ù‡Ø§ Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ -->
            <!-- Ø¨Ø®Ø´ 4: Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø§ YOLO -->
            <div class="card shadow-lg mb-5">
                <div class="card-header bg-success text-white">
                    <h4 class="mb-0"><i class="bi bi-bounding-box me-2"></i>ğŸ“¦ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø§ YOLO</h4>
                </div>
                <div class="card-body">
                    <p class="text-muted mb-4">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ YOLO</p>
                    
                    <pre class="bg-light p-4 rounded" style="direction: ltr; text-align: left; max-height: 400px; overflow-y: auto;"><code class="language-python">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Real-time Object Detection with YOLO
Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ YOLO
"""

import cv2
import numpy as np
import time
import requests
import json
from datetime import datetime

class YOLODetector:
    """Ú©Ù„Ø§Ø³ ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø§ YOLO"""
    
    def __init__(self, config_path, weights_path, classes_path, confidence_threshold=0.5, nms_threshold=0.4):
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        
        # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ YOLO
        self.net = cv2.dnn.readNetFromDarknet(config_path, weights_path)
        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
        
        # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ø§Ù… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        with open(classes_path, 'r') as f:
            self.classes = [line.strip() for line in f.readlines()]
        
        # Ú¯Ø±ÙØªÙ† Ù†Ø§Ù… Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
        layer_names = self.net.getLayerNames()
        self.output_layers = [layer_names[i - 1] for i in self.net.getUnconnectedOutLayers()]
        
        # Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        self.colors = np.random.uniform(0, 255, size=(len(self.classes), 3))
        
        # Ø¢Ù…Ø§Ø± ØªØ´Ø®ÛŒØµ
        self.detection_history = []
        self.frame_count = 0
        self.start_time = time.time()
    
    def detect_objects(self, frame):
        """ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± ÙØ±ÛŒÙ…"""
        self.frame_count += 1
        
        height, width = frame.shape[:2]
        
        # Ø§ÛŒØ¬Ø§Ø¯ blob Ø§Ø² ØªØµÙˆÛŒØ±
        blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)
        self.net.setInput(blob)
        
        # Ø§Ø¬Ø±Ø§ÛŒ forward pass
        start_time = time.time()
        layer_outputs = self.net.forward(self.output_layers)
        inference_time = time.time() - start_time
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªØ§ÛŒØ¬
        boxes = []
        confidences = []
        class_ids = []
        
        for output in layer_outputs:
            for detection in output:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                
                if confidence > self.confidence_threshold:
                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø®ØªØµØ§Øª Ø¬Ø¹Ø¨Ù‡ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†Ù†Ø¯Ù‡
                    center_x = int(detection[0] * width)
                    center_y = int(detection[1] * height)
                    w = int(detection[2] * width)
                    h = int(detection[3] * height)
                    
                    # Ù…Ø®ØªØµØ§Øª Ú¯ÙˆØ´Ù‡ Ø¨Ø§Ù„Ø§-Ú†Ù¾
                    x = int(center_x - w / 2)
                    y = int(center_y - h / 2)
                    
                    boxes.append([x, y, w, h])
                    confidences.append(float(confidence))
                    class_ids.append(class_id)
        
        # Ø§Ø¹Ù…Ø§Ù„ Non-Maximum Suppression
        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.confidence_threshold, self.nms_threshold)
        
        detected_objects = []
        
        if len(indices) > 0:
            for i in indices.flatten():
                x, y, w, h = boxes[i]
                confidence = confidences[i]
                class_id = class_ids[i]
                
                detected_objects.append({
                    'class_id': class_id,
                    'class_name': self.classes[class_id],
                    'confidence': confidence,
                    'bbox': (x, y, w, h),
                    'center': (x + w//2, y + h//2)
                })
        
        # Ø«Ø¨Øª Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self._record_detection(detected_objects, inference_time)
        
        return detected_objects, inference_time
    
    def _record_detection(self, detected_objects, inference_time):
        """Ø«Ø¨Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ´Ø®ÛŒØµ"""
        detection_data = {
            'timestamp': datetime.now().isoformat(),
            'frame_number': self.frame_count,
            'objects_detected': len(detected_objects),
            'inference_time': inference_time,
            'objects': detected_objects
        }
        
        self.detection_history.append(detection_data)
    
    def draw_detections(self, frame, detected_objects):
        """Ø±Ø³Ù… Ø¬Ø¹Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†Ù†Ø¯Ù‡ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±"""
        for obj in detected_objects:
            x, y, w, h = obj['bbox']
            class_name = obj['class_name']
            confidence = obj['confidence']
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ù†Ú¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³
            color = self.colors[obj['class_id']]
            
            # Ø±Ø³Ù… Ø¬Ø¹Ø¨Ù‡ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†Ù†Ø¯Ù‡
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            
            # Ø±Ø³Ù… Ø¨Ø±Ú†Ø³Ø¨
            label = f"{class_name}: {confidence:.2f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            
            # Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ø¨Ø±Ú†Ø³Ø¨
            cv2.rectangle(frame, (x, y - label_size[1] - 10), 
                         (x + label_size[0], y), color, -1)
            
            # Ù…ØªÙ† Ø¨Ø±Ú†Ø³Ø¨
            cv2.putText(frame, label, (x, y - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
            
            # Ø±Ø³Ù… Ù…Ø±Ú©Ø² Ø´ÛŒØ¡
            center_x, center_y = obj['center']
            cv2.circle(frame, (center_x, center_y), 3, color, -1)
    
    def draw_analytics(self, frame, detected_objects, inference_time):
        """Ø±Ø³Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù†Ø§Ù„ÛŒØªÛŒÚ©Ø³"""
        # Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ù†ÛŒÙ…Ù‡ Ø´ÙØ§Ù
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (350, 120), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)
        
        # Ø¢Ù…Ø§Ø± Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ
        current_time = time.time()
        runtime = current_time - self.start_time
        fps = self.frame_count / runtime if runtime > 0 else 0
        
        texts = [
            f"Objects Detected: {len(detected_objects)}",
            f"Inference Time: {inference_time*1000:.1f}ms",
            f"FPS: {fps:.1f}",
            f"Frame: {self.frame_count}",
            f"Runtime: {runtime:.1f}s"
        ]
        
        for i, text in enumerate(texts):
            y_pos = 35 + (i * 20)
            cv2.putText(frame, text, (20, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        class_counts = {}
        for obj in detected_objects:
            class_name = obj['class_name']
            class_counts[class_name] = class_counts.get(class_name, 0) + 1
        
        y_pos = 140
        for class_name, count in list(class_counts.items())[:5]:  # Ù†Ù…Ø§ÛŒØ´ 5 Ú©Ù„Ø§Ø³ Ø§ÙˆÙ„
            text = f"{class_name}: {count}"
            cv2.putText(frame, text, (20, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            y_pos += 15
    
    def get_detection_statistics(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± ØªØ´Ø®ÛŒØµ"""
        total_objects = sum(len(detection['objects']) for detection in self.detection_history)
        avg_inference_time = np.mean([detection['inference_time'] 
                                    for detection in self.detection_history[-100:]])  # 100 ÙØ±ÛŒÙ… Ø¢Ø®Ø±
        
        stats = {
            'total_frames': self.frame_count,
            'total_objects_detected': total_objects,
            'average_inference_time': avg_inference_time,
            'objects_per_frame': total_objects / self.frame_count if self.frame_count > 0 else 0,
            'runtime': time.time() - self.start_time
        }
        
        return stats
    
    def save_detection_report(self, filename='yolo_detection_report.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ ØªØ´Ø®ÛŒØµ"""
        report = {
            'system_info': {
                'model': 'YOLO Object Detection',
                'confidence_threshold': self.confidence_threshold,
                'total_frames_processed': self.frame_count,
                'report_generated': datetime.now().isoformat()
            },
            'detection_history': self.detection_history[-1000:],  # 1000 ØªØ´Ø®ÛŒØµ Ø¢Ø®Ø±
            'statistics': self.get_detection_statistics()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡"""
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø§ YOLO...")
    print("ğŸ“¦ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„...")
    
    # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ YOLO (Ø¨Ø§ÛŒØ¯ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´ÙˆÙ†Ø¯)
    config_path = 'yolov3.cfg'
    weights_path = 'yolov3.weights'
    classes_path = 'coco.names'
    
    try:
        # Ø§ÛŒØ¬Ø§Ø¯ ØªØ´Ø®ÛŒØµâ€ŒØ¯Ù‡Ù†Ø¯Ù‡
        detector = YOLODetector(config_path, weights_path, classes_path)
        print("âœ… Ù…Ø¯Ù„ YOLO Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø´Ø¯")
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÙˆØ±Ø¨ÛŒÙ†
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ†")
            return
        
        print("ğŸ“¹ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
        print("ğŸ¯ Ø¯Ø± Ø­Ø§Ù„ ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡... (Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ú©Ù„ÛŒØ¯ 'q' Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯)")
        
        while True:
            ret, frame = cap.read()
            
            if not ret:
                print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ±ÛŒÙ…")
                break
            
            # ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡
            detected_objects, inference_time = detector.detect_objects(frame)
            
            # Ø±Ø³Ù… Ù†ØªØ§ÛŒØ¬
            detector.draw_detections(frame, detected_objects)
            detector.draw_analytics(frame, detected_objects, inference_time)
            
            # Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ±
            cv2.imshow('YOLO Object Detection', frame)
            
            # Ø®Ø±ÙˆØ¬ Ø¨Ø§ Ú©Ù„ÛŒØ¯ 'q'
            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("\nğŸ›‘ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ…...")
                break
                
    except FileNotFoundError as e:
        print(f"âŒ ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ YOLO ÛŒØ§ÙØª Ù†Ø´Ø¯: {e}")
        print("ğŸ“¥ Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ yolov3.cfg, yolov3.weights Ùˆ coco.names Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…: {e}")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        detector.save_detection_report()
        print("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ ØªØ´Ø®ÛŒØµ Ø¯Ø± ÙØ§ÛŒÙ„ 'yolo_detection_report.json' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
        final_stats = detector.get_detection_statistics()
        print("\nğŸ“ˆ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ…:")
        print(f"   ğŸ¯ Ú©Ù„ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {final_stats['total_frames']}")
        print(f"   ğŸ“¦ Ú©Ù„ Ø§Ø´ÛŒØ§Ø¡ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: {final_stats['total_objects_detected']}")
        print(f"   âš¡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† inference: {final_stats['average_inference_time']*1000:.1f}ms")
        print(f"   ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± ÙØ±ÛŒÙ…: {final_stats['objects_per_frame']:.2f}")
        print(f"   â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {final_stats['runtime']:.1f} Ø«Ø§Ù†ÛŒÙ‡")

if __name__ == "__main__":
    main()
                    </code></pre>
                </div>
            </div>

        </div>
    </div>

    <!-- Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ -->
    <div class="row mt-5">
        <div class="col-md-6">
            <div class="card border-primary">
                <div class="card-body">
                    <h5 class="card-title text-primary">ğŸ“š Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡</h5>
                    <ul class="list-unstyled">
                        <li><strong>OpenCV:</strong> Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± Ùˆ ÙˆÛŒØ¯ÛŒÙˆ</li>
                        <li><strong>NumPy:</strong> Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø¢Ø±Ø§ÛŒÙ‡â€ŒÙ‡Ø§</li>
                        <li><strong>Matplotlib:</strong> Ø¨ØµØ±ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§</li>
                        <li><strong>TensorFlow/PyTorch:</strong> ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚</li>
                        <li><strong>YOLO/SSD:</strong> Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡</li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="col-md-6">
            <div class="card border-info">
                <div class="card-body">
                    <h5 class="card-title text-info">âš¡ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡</h5>
                    <ul class="list-unstyled">
                        <li>ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ùˆ Ø§Ø´ÛŒØ§Ø¡ Ù¾ÛŒØ´Ø±ÙØªÙ‡</li>
                        <li>ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ùˆ Ø§ÙÚ©Øªâ€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ</li>
                        <li>Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯</li>
                        <li>ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ YOLO</li>
                        <li>Ø¢Ù†Ø§Ù„ÛŒØ² Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <!-- CTA Section -->
    <div class="row mt-5">
        <div class="col-12 text-center">
            <div class="card bg-gradient-primary text-white shadow-lg">
                <div class="card-body py-5">
                    <h3 class="fw-bold mb-3">ğŸ¯ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ùˆ Ú©Ø¯Ù‡Ø§ÛŒ Ú©Ø§Ù…Ù„</h3>
                    <p class="mb-4">Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ùˆ Ú©Ø¯Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ Ù…Ø®Ø²Ù† GitHub Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯</p>
                    <div class="d-flex flex-wrap justify-content-center gap-3">
                        <a href="{% url 'projects' %}" class="btn btn-light btn-lg">
                            <i class="bi bi-arrow-left me-2"></i>Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§
                        </a>
                        <a href="https://github.com/Amirmohammdkhaki" class="btn btn-outline-light btn-lg" target="_blank">
                            <i class="bi bi-github me-2"></i>Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø¯ Ú©Ø§Ù…Ù„
                        </a>
                        <a href="{% url 'post_list' %}" class="btn btn-outline-light btn-lg">
                            <i class="bi bi-journal-code me-2"></i>Ù…Ù‚Ø§Ù„Ø§Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<style>
    .card {
        border-radius: 1rem;
        transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    .card:hover {
        transform: translateY(-5px);
        box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
    }
    
    pre {
        background: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 0.5rem;
        font-size: 0.85rem;
        line-height: 1.4;
    }
    
    code {
        color: #d63384;
        font-family: 'Courier New', monospace;
    }
    
    .bg-gradient-primary {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    }
    
    .card-header {
        border-radius: 1rem 1rem 0 0 !important;
    }
</style>

<script>
// Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ø±Ø§ÛŒ Ù‡Ø§ÛŒÙ„Ø§ÛŒØª Ú©Ø¯
document.addEventListener('DOMContentLoaded', function() {
    // Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù‚Ø§Ø¨Ù„ÛŒØª Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯
    const codeBlocks = document.querySelectorAll('pre code');
    
    codeBlocks.forEach((codeBlock) => {
        const pre = codeBlock.parentElement;
        
        // Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ú©Ù…Ù‡ Ú©Ù¾ÛŒ
        const copyButton = document.createElement('button');
        copyButton.className = 'btn btn-sm btn-outline-secondary position-absolute';
        copyButton.style.top = '10px';
        copyButton.style.right = '10px';
        copyButton.innerHTML = '<i class="bi bi-clipboard"></i>';
        copyButton.title = 'Ú©Ù¾ÛŒ Ú©Ø¯';
        
        pre.style.position = 'relative';
        pre.appendChild(copyButton);
        
        copyButton.addEventListener('click', async () => {
            try {
                await navigator.clipboard.writeText(codeBlock.textContent);
                copyButton.innerHTML = '<i class="bi bi-check"></i>';
                copyButton.className = 'btn btn-sm btn-success position-absolute';
                
                setTimeout(() => {
                    copyButton.innerHTML = '<i class="bi bi-clipboard"></i>';
                    copyButton.className = 'btn btn-sm btn-outline-secondary position-absolute';
                }, 2000);
            } catch (err) {
                console.error('Ø®Ø·Ø§ Ø¯Ø± Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù†: ', err);
            }
        });
    });
    
    // Ø§Ù†ÛŒÙ…ÛŒØ´Ù† Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§
    const cards = document.querySelectorAll('.card');
    const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
            if (entry.isIntersecting) {
                entry.target.style.opacity = '1';
                entry.target.style.transform = 'translateY(0)';
            }
        });
    }, { threshold: 0.1 });
    
    cards.forEach(card => {
        card.style.opacity = '0';
        card.style.transform = 'translateY(20px)';
        card.style.transition = 'all 0.6s ease';
        observer.observe(card);
    });
});
</script>
{% endblock %}